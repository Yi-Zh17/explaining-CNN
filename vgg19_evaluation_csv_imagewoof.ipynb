{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b90ba409-66ce-4eb2-96d2-b3c95502fa3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T12:43:46.229497Z",
     "iopub.status.busy": "2025-04-15T12:43:46.229266Z",
     "iopub.status.idle": "2025-04-15T12:43:48.710157Z",
     "shell.execute_reply": "2025-04-15T12:43:48.709590Z",
     "shell.execute_reply.started": "2025-04-15T12:43:46.229480Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Image-related utilities\n",
    "from torchvision.io import decode_image, read_image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Import models\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torchvision.models import vgg11, VGG11_Weights\n",
    "\n",
    "# Dataset\n",
    "from torchvision.datasets import Imagenette, ImageFolder\n",
    "\n",
    "# Plotting utility\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d13d2762-a36e-4e9e-8092-688c9519a0fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T12:43:48.711350Z",
     "iopub.status.busy": "2025-04-15T12:43:48.711082Z",
     "iopub.status.idle": "2025-04-15T12:43:48.892984Z",
     "shell.execute_reply": "2025-04-15T12:43:48.892507Z",
     "shell.execute_reply.started": "2025-04-15T12:43:48.711335Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Read imagenette data into data loader\n",
    "#Process image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to VGG19 input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Same normalization as ImageNet\n",
    "])\n",
    "imagewoof_data = ImageFolder(root='/home/yi/Downloads/imagewoof2/val', transform=transform)\n",
    "data_loader = torch.utils.data.DataLoader(imagewoof_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71fbde3b-0160-4473-b2f2-8de2014bbd1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T12:43:48.893778Z",
     "iopub.status.busy": "2025-04-15T12:43:48.893591Z",
     "iopub.status.idle": "2025-04-15T12:43:50.771564Z",
     "shell.execute_reply": "2025-04-15T12:43:50.771124Z",
     "shell.execute_reply.started": "2025-04-15T12:43:48.893764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrained model\n",
    "# PATH = 'vgg19_imagewoof.pth'\n",
    "# model = vgg19()\n",
    "# PATH = \"vgg16_Imagewoof.pth\"\n",
    "# model = vgg16()\n",
    "PATH = \"vgg11_Imagewoof.pth\"\n",
    "model = vgg11()\n",
    "\n",
    "model.classifier[6] = torch.nn.Linear(in_features=4096, out_features=10)\n",
    "model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0ef80b-8123-489d-ab7c-ea00b2e801da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T12:43:50.772278Z",
     "iopub.status.busy": "2025-04-15T12:43:50.772075Z",
     "iopub.status.idle": "2025-04-15T12:44:41.576667Z",
     "shell.execute_reply": "2025-04-15T12:44:41.576269Z",
     "shell.execute_reply.started": "2025-04-15T12:43:50.772263Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████| 3929/3929 [00:50<00:00, 77.37images/s]\n"
     ]
    }
   ],
   "source": [
    "img_path = []\n",
    "true_label = []\n",
    "predicted_label = []\n",
    "confidence = []\n",
    "true_confidence = []\n",
    "correctness = []\n",
    "\n",
    "image_paths = [data_loader.dataset.samples[i][0] for i in range(len(data_loader.dataset))]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (img, label) in enumerate(tqdm(data_loader, desc=\"Evaluating\", unit=\"images\")):\n",
    "        # Retrieve image\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        # Prediction\n",
    "        prediction = model(img).squeeze(0).softmax(0)\n",
    "        class_id = prediction.argmax().item()\n",
    "        score = prediction[class_id].item()\n",
    "        true_score = prediction[label.item()].item()\n",
    "        \n",
    "        # Correct?\n",
    "        #category_name = weights.meta[\"categories\"][class_id]\n",
    "        \n",
    "        # Extract class names from the dataset\n",
    "        id_to_class = {v: k for k, v in imagewoof_data.class_to_idx.items()}  # Reverse mapping\n",
    "        # Get predicted category name\n",
    "        category_name = id_to_class[class_id]\n",
    "        # Check if prediction is correct\n",
    "        correct = 'y' if category_name == id_to_class[label.item()] else 'n'\n",
    "        \n",
    "        # Log\n",
    "        img_path.append(image_paths[idx])\n",
    "        true_label.append(id_to_class[label.item()])\n",
    "        predicted_label.append(category_name)\n",
    "        confidence.append(score)\n",
    "        true_confidence.append(true_score)\n",
    "        correctness.append(correct)\n",
    "\n",
    "# Dictionary of logs\n",
    "dict = {'image': img_path, 'true label': true_label, 'predicted label': predicted_label,\n",
    "        'confidence score': confidence, 'true confidence score': true_confidence, 'correct': correctness}\n",
    "df = pd.DataFrame(dict)\n",
    "# df.to_csv('vgg19_results_retrained_imagewoof.csv')\n",
    "# df.to_csv('vgg16_results_retrained_imagewoof.csv')\n",
    "df.to_csv('vgg11_results_retrained_imagewoof.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e088edc-adbe-49ff-8456-97aa61ec8d6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T12:44:41.577338Z",
     "iopub.status.busy": "2025-04-15T12:44:41.577182Z",
     "iopub.status.idle": "2025-04-15T12:44:41.582582Z",
     "shell.execute_reply": "2025-04-15T12:44:41.582112Z",
     "shell.execute_reply.started": "2025-04-15T12:44:41.577321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy:  0.8962\n"
     ]
    }
   ],
   "source": [
    "# Print evaluation accuracy\n",
    "print('Evaluation accuracy: ', round(len(df[df['correct'] == 'y']) / len(df), 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
