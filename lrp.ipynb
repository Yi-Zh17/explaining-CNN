{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca024db-0240-4bc1-a901-6169b98bdf2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:15:25.268902Z",
     "iopub.status.busy": "2025-03-12T09:15:25.268784Z",
     "iopub.status.idle": "2025-03-12T09:15:27.789860Z",
     "shell.execute_reply": "2025-03-12T09:15:27.789266Z",
     "shell.execute_reply.started": "2025-03-12T09:15:25.268888Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Image-related utilities\n",
    "from torchvision.io import decode_image, read_image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Import models\n",
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "\n",
    "# Dataset\n",
    "from torchvision.datasets import Imagenette\n",
    "\n",
    "# LRP package\n",
    "from src.lrp import LRPModel\n",
    "from src.data import get_data_loader\n",
    "\n",
    "# Utils\n",
    "import argparse\n",
    "import time\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6eb19d0-a73e-48c0-b829-8ed3f44e8ef2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:15:27.791146Z",
     "iopub.status.busy": "2025-03-12T09:15:27.790952Z",
     "iopub.status.idle": "2025-03-12T09:15:27.793853Z",
     "shell.execute_reply": "2025-03-12T09:15:27.793490Z",
     "shell.execute_reply.started": "2025-03-12T09:15:27.791131Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define custom colormap\n",
    "colors = [\"white\", \"red\"]  # Transition from white to red\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"white_red\", colors, N=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfda1d7f-2d4d-4c97-8496-7285707668b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:15:27.794546Z",
     "iopub.status.busy": "2025-03-12T09:15:27.794295Z",
     "iopub.status.idle": "2025-03-12T09:15:27.806874Z",
     "shell.execute_reply": "2025-03-12T09:15:27.806433Z",
     "shell.execute_reply.started": "2025-03-12T09:15:27.794532Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_relevance_scores(\n",
    "    x: torch.tensor, r: torch.tensor, name: str\n",
    ") -> None:\n",
    "    \"\"\"Plots results from layer-wise relevance propagation next to original image.\n",
    "\n",
    "    Method currently accepts only a batch size of one.\n",
    "\n",
    "    Args:\n",
    "        x: Original image.\n",
    "        r: Relevance scores for original image.\n",
    "        name: Image name.\n",
    "        config: Argparse namespace object.\n",
    "\n",
    "    \"\"\"\n",
    "    output_dir = \"./output/\"\n",
    "\n",
    "    max_fig_size = 20\n",
    "\n",
    "    _, _, img_height, img_width = x.shape\n",
    "    max_dim = max(img_height, img_width)\n",
    "    fig_height, fig_width = (\n",
    "        max_fig_size * img_height / max_dim,\n",
    "        max_fig_size * img_width / max_dim,\n",
    "    )\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(fig_width, fig_height))\n",
    "\n",
    "    x = x[0].squeeze().permute(1, 2, 0).detach().cpu()\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    x = (x - x_min) / (x_max - x_min)\n",
    "    axes[0].imshow(x)\n",
    "    axes[0].set_axis_off()\n",
    "\n",
    "    r_min = r.min()\n",
    "    r_max = r.max()\n",
    "    r = (r - r_min) / (r_max - r_min)\n",
    "    axes[1].imshow(r, cmap='hot')\n",
    "    axes[1].set_axis_off()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/image_{name}.png\", bbox_inches=\"tight\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c1f47f-f9d3-4858-b05a-f81f6f4826af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:15:27.807478Z",
     "iopub.status.busy": "2025-03-12T09:15:27.807357Z",
     "iopub.status.idle": "2025-03-12T09:15:27.811090Z",
     "shell.execute_reply": "2025-03-12T09:15:27.810745Z",
     "shell.execute_reply.started": "2025-03-12T09:15:27.807466Z"
    }
   },
   "outputs": [],
   "source": [
    "def per_image_lrp(model):\n",
    "    \"\"\"Test function that plots heatmaps for images placed in the input folder.\n",
    "\n",
    "    Images have to be placed in their corresponding class folders.\n",
    "\n",
    "    Args:\n",
    "        config: Argparse namespace object.\n",
    "\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    print(f\"Using: {device}\\n\")\n",
    "\n",
    "    data_loader = get_data_loader()\n",
    "    \n",
    "    model = model\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    lrp_model = LRPModel(model=model, top_k=0.02)\n",
    "\n",
    "    for i, (x, y) in enumerate(data_loader):\n",
    "        x = x.to(device)\n",
    "        # y = y.to(device)  # here not used as method is unsupervised.\n",
    "\n",
    "        t0 = time.time()\n",
    "        r = lrp_model.forward(x)\n",
    "        print(\"{time:.2f} FPS\".format(time=(1.0 / (time.time() - t0))))\n",
    "\n",
    "        plot_relevance_scores(x=x, r=r, name=str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c3664c-4964-43d3-9fad-c5f48eb20212",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:15:27.811851Z",
     "iopub.status.busy": "2025-03-12T09:15:27.811524Z",
     "iopub.status.idle": "2025-03-12T09:15:43.344147Z",
     "shell.execute_reply": "2025-03-12T09:15:43.343576Z",
     "shell.execute_reply.started": "2025-03-12T09:15:27.811836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n",
      "\n",
      "3.46 FPS\n",
      "5.44 FPS\n",
      "7.93 FPS\n",
      "7.47 FPS\n",
      "6.71 FPS\n",
      "7.38 FPS\n",
      "11.81 FPS\n",
      "7.72 FPS\n",
      "6.50 FPS\n",
      "6.78 FPS\n",
      "7.71 FPS\n",
      "11.07 FPS\n",
      "7.45 FPS\n",
      "10.34 FPS\n",
      "7.63 FPS\n",
      "7.33 FPS\n",
      "7.73 FPS\n",
      "7.39 FPS\n",
      "7.06 FPS\n",
      "10.07 FPS\n"
     ]
    }
   ],
   "source": [
    "# Pre-trained model\n",
    "# per_image_lrp(vgg19(weights=VGG19_Weights.DEFAULT))\n",
    "\n",
    "# Retrained model\n",
    "PATH = 'vgg19_imagenette.pth'\n",
    "model = vgg19()\n",
    "model.classifier[6] = torch.nn.Linear(in_features=4096, out_features=10)\n",
    "model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "per_image_lrp(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
